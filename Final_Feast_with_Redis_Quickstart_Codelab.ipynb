{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final - Feast with Redis Quickstart Codelab",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5JTeKfCVBZf"
      },
      "source": [
        "# ML Feature Store Quickstart Tutorial - Getting Started with Feast using Redis\n",
        "\n",
        "This tutorial provides a step-by-step **Feast for Redis quickstart** that walks you through an end-to-end example of using Feast with Redis as its online feature store for machine learning. It is based on the Feast Quickstart tutorial ([here](https://docs.feast.dev/getting-started/quickstart))), but instead of using the default online store, it **uses the Redis online store** for delivering real-time predictions at scale. If you are not familiar with Feast or Redis, then the fastest way to get started with Feast using Redis is through this tutorial. For a high-level introduction to Feature Stores and Feast using Redis, please refer to [this blog article](https://redis.com/blog/building-feature-stores-with-redis-introduction-to-feast-with-redis). More detailed information on Redis and Feast, as well as additional resources, are available at the end of this tutorial. \n",
        "\n",
        "In this tutorial you will:\n",
        "\n",
        "1.   Deploy a local feature store with a Parquet file offline store and Redis online store.\n",
        "2.   Build a training dataset using the demo time series features from the Parquet files.\n",
        "3.   Materialize (load) feature values from the offline store into the Redis online store.\n",
        "4.   Read the latest features from the Redis online store for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Feast in a nutshell:**\n",
        "\n",
        "\n",
        "\n",
        "> Feast (**Fea**ture **st**ore) is an open source feature store and [is part of the Linux Foundation AI & Data Foundation](//https://lfaidata.foundation/blog/2020/11/10/feast-joins-lf-ai-data-as-new-incubation-project/). It can serve feature data to models from a low-latency online store (for real-time serving) or an offline store (for model training or batch serving). It also provides a central registry so **machine learning engineers** and **data scientists** can discover the relevant features for ML use cases. Feast is a Python library + optional CLI. You can install Feast using pip, as will be described soon in this tutorial.\n"
      ],
      "metadata": {
        "id": "6ZuVP9m4R1CO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Redis in a nutshell:**\n",
        "\n",
        "> Redis is an open source (BSD licensed), **in-memory** data structure store, used as a database, cache, and message broker. [Redis](https://redis.com) provides data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, geospatial indexes, and streams. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence, and provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.\n"
      ],
      "metadata": {
        "id": "n4lIvqyPaQzZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jvSeTC3PihH"
      },
      "source": [
        "## Demo Scenario and Tutorial Steps\n",
        "In this tutorial, we use feature stores to generate training data and power online model inference for a **ride-sharing driver satisfaction** prediction model. In the demo data scenario: \n",
        "We have surveyed some drivers to determine how satisfied they are with their experience using a ride-sharing app. \n",
        "We want to generate predictions for driver satisfaction for the rest of the users so we can reach out to potentially dissatisfied users.\n",
        "\n",
        "Tutorial Steps:\n",
        "1.   Install Feast and Redis and run Redis-Server in the background\n",
        "2.   Create a feature repository and configure Redis as the online store\n",
        "3.   Register feature definitions and deploy your feature store\n",
        "4.   Generate training data\n",
        "5.   Load features into your Redis online store\n",
        "6.   Fetch feature vectors for inference from Redis online store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Y997DzvOMI"
      },
      "source": [
        "## Step 1: Install Feast and Redis and run Redis-Server in the background\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 1a**: Install Feast for Redis (and Pygments for pretty printing) using pip"
      ],
      "metadata": {
        "id": "lzxxt2_7nxre"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXNMAAJKQPG5",
        "outputId": "b82672a2-d99b-4c37-b877-7ed64756eec8"
      },
      "source": [
        "%%sh\n",
        "pip install feast[redis] -U -q\n",
        "pip install Pygments -q\n",
        "echo \"Please restart your runtime now (Runtime -> Restart runtime). This ensures that the correct dependencies are loaded.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please restart your runtime now (Runtime -> Restart runtime). This ensures that the correct dependencies are loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "sOX_LwjaAhKz"
      },
      "source": [
        "**Reminder**: Please restart your runtime after installing Feast (Runtime -> Restart runtime). This ensures that the correct dependencies are loaded.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 1b**: Install Redis using pip"
      ],
      "metadata": {
        "id": "oWM-R86Hokq7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hR-YodOenoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dc9ac3-fb2f-4856-8ffe-6163e873165f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting redis\n",
            "  Downloading redis-4.0.2-py3-none-any.whl (119 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▊                             | 10 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 20 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30 kB 33.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 40 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 51 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 61 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 71 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 81 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 92 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 102 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 112 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 119 kB 23.9 MB/s \n",
            "\u001b[?25hCollecting redis-server\n",
            "  Downloading redis_server-6.0.9-202010301343-cp37-cp37m-manylinux2010_x86_64.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 33.7 MB/s \n",
            "\u001b[?25hCollecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->redis) (1.13.3)\n",
            "Installing collected packages: deprecated, redis-server, redis\n",
            "Successfully installed deprecated-1.2.13 redis-4.0.2 redis-server-6.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install redis redis-server"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Additional information on alternative ways for installing Redis can be found here: https://redis.io/download#installation. Additional configuration information can be found in the Redis Quick Start guide (https://redis.io/topics/quickstart)."
      ],
      "metadata": {
        "id": "11SFkRnqch4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 1c**: Start Redis Server"
      ],
      "metadata": {
        "id": "9IzJW3cShFZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import redis_server\n",
        "\n",
        "subprocess.Popen([redis_server.REDIS_SERVER_PATH]) "
      ],
      "metadata": {
        "id": "-MfrA0Ykepy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1159ac41-8a8f-44d3-b06f-2d0f8a5036d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<subprocess.Popen at 0x7fc5c7a9ba10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZetvs5xx4GP"
      },
      "source": [
        "## Step 2: Create a feature repository and configure Redis as the online store\n",
        "\n",
        "A feature repository is a directory that contains the configuration of the feature store and individual features. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Step 2a**: Create a feature repository\n",
        "\n",
        "The easiest way to create a new feature repository to use the `feast init` command. This creates a scaffolding with initial demo data."
      ],
      "metadata": {
        "id": "YjqSs4_mvA45"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhirSkgUvYau",
        "outputId": "cb28f02e-1175-427f-9e0c-f927d1b42c1e"
      },
      "source": [
        "!feast init feature_repo\n",
        "%cd feature_repo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feast is an open source project that collects anonymized error reporting and usage statistics. To opt out or learn more see https://docs.feast.dev/reference/usage\n",
            "\n",
            "Creating a new Feast repository in \u001b[1m\u001b[32m/content/feature_repo\u001b[0m.\n",
            "\n",
            "/content/feature_repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdTASZPvyKCe"
      },
      "source": [
        "\n",
        "Let's take a look at the demo repo itself. It breaks down into\n",
        "\n",
        "\n",
        "*   `data/` contains raw demo parquet data\n",
        "*   `example.py` contains demo feature definitions\n",
        "*   `feature_store.yaml` contains a demo setup configuring where data sources are\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jXuzt4ovzA3",
        "outputId": "48663739-7282-4750-e906-7507b56da8c6"
      },
      "source": [
        "!ls -R"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".:\n",
            "data  example.py  feature_store.yaml\n",
            "\n",
            "./data:\n",
            "driver_stats.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJk_WNsbeUP6"
      },
      "source": [
        "### **Step 2b**: Configure Redis as the online store in the YAML configuration file\n",
        "To configure Redis as the online store we need to set the `type` and `connection_string` values for `online_store`  in `feature_store.yaml` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b941d1-ef1e-4f5e-ac6f-6354f0ae8200",
        "id": "Y7dLeBuSg1Kc"
      },
      "source": [
        "%%writefile feature_store.yaml\n",
        "project: feature_repo\n",
        "registry: data/registry.db\n",
        "provider: local\n",
        "online_store:\n",
        "    type: redis\n",
        "    connection_string: localhost:6379"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting feature_store.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `provider` defines where the raw data exists (for generating training data and feature values for serving) in this demo, locally. The `online_store` defines where to materialize ( load) feature values in the online store database (for serving).\n",
        "\n",
        "Note that the above configuration is different from the default YAML file provided for the tutorial that instead uses the default online store.\n",
        "\n",
        "So by adding these two lines for `online_store` (`type: redis, connection_string: localhost:6379`) in the YAML file per the above, Feast is then able to read and write from Redis as its online store. Redis Online Store is part of the Feast core code, and as such, Feast knows how to use Redis out-of-the-box.\n",
        "\n"
      ],
      "metadata": {
        "id": "PA_nqlLYkK25"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NS4INL5n7ze"
      },
      "source": [
        "### **Step 2c**: Inspect feature definitions\n",
        "Let’s take a look at the demo feature definitions at `example.py`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPqXCoNpL0SX",
        "outputId": "6234f8aa-7342-4926-f7ca-9da02fc2e178"
      },
      "source": [
        "!pygmentize -f terminal16m example.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;2;64;128;128m# This is an example feature definition file\u001b[39m\n",
            "\n",
            "\u001b[38;2;0;128;0;01mfrom\u001b[39;00m \u001b[38;2;0;0;255;01mgoogle.protobuf.duration_pb2\u001b[39;00m \u001b[38;2;0;128;0;01mimport\u001b[39;00m Duration\n",
            "\n",
            "\u001b[38;2;0;128;0;01mfrom\u001b[39;00m \u001b[38;2;0;0;255;01mfeast\u001b[39;00m \u001b[38;2;0;128;0;01mimport\u001b[39;00m Entity, Feature, FeatureView, FileSource, ValueType\n",
            "\n",
            "\u001b[38;2;64;128;128m# Read data from parquet files. Parquet is convenient for local development mode. For\u001b[39m\n",
            "\u001b[38;2;64;128;128m# production, you can use your favorite DWH, such as BigQuery. See Feast documentation\u001b[39m\n",
            "\u001b[38;2;64;128;128m# for more info.\u001b[39m\n",
            "driver_hourly_stats \u001b[38;2;102;102;102m=\u001b[39m FileSource(\n",
            "    path\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33m/content/feature_repo/data/driver_stats.parquet\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m,\n",
            "    event_timestamp_column\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mevent_timestamp\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m,\n",
            "    created_timestamp_column\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mcreated\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m,\n",
            ")\n",
            "\n",
            "\u001b[38;2;64;128;128m# Define an entity for the driver. You can think of entity as a primary key used to\u001b[39m\n",
            "\u001b[38;2;64;128;128m# fetch features.\u001b[39m\n",
            "driver \u001b[38;2;102;102;102m=\u001b[39m Entity(name\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mdriver_id\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m, value_type\u001b[38;2;102;102;102m=\u001b[39mValueType\u001b[38;2;102;102;102m.\u001b[39mINT64, description\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mdriver id\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m,)\n",
            "\n",
            "\u001b[38;2;64;128;128m# Our parquet files contain sample data that includes a driver_id column, timestamps and\u001b[39m\n",
            "\u001b[38;2;64;128;128m# three feature column. Here we define a Feature View that will allow us to serve this\u001b[39m\n",
            "\u001b[38;2;64;128;128m# data to our model online.\u001b[39m\n",
            "driver_hourly_stats_view \u001b[38;2;102;102;102m=\u001b[39m FeatureView(\n",
            "    name\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mdriver_hourly_stats\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m,\n",
            "    entities\u001b[38;2;102;102;102m=\u001b[39m[\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mdriver_id\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m],\n",
            "    ttl\u001b[38;2;102;102;102m=\u001b[39mDuration(seconds\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m86400\u001b[39m \u001b[38;2;102;102;102m*\u001b[39m \u001b[38;2;102;102;102m1\u001b[39m),\n",
            "    features\u001b[38;2;102;102;102m=\u001b[39m[\n",
            "        Feature(name\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mconv_rate\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m, dtype\u001b[38;2;102;102;102m=\u001b[39mValueType\u001b[38;2;102;102;102m.\u001b[39mFLOAT),\n",
            "        Feature(name\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33macc_rate\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m, dtype\u001b[38;2;102;102;102m=\u001b[39mValueType\u001b[38;2;102;102;102m.\u001b[39mFLOAT),\n",
            "        Feature(name\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mavg_daily_trips\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m, dtype\u001b[38;2;102;102;102m=\u001b[39mValueType\u001b[38;2;102;102;102m.\u001b[39mINT64),\n",
            "    ],\n",
            "    online\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mTrue\u001b[39m,\n",
            "    batch_source\u001b[38;2;102;102;102m=\u001b[39mdriver_hourly_stats,\n",
            "    tags\u001b[38;2;102;102;102m=\u001b[39m{},\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnMlk4zshywp"
      },
      "source": [
        "###**Step 2d:** Inspect the raw data\n",
        "\n",
        "The raw feature data we have in this demo is stored in a local parquet file. The dataset captures hourly stats of a driver in a ride-sharing app."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "sIF2lO59dwzi",
        "outputId": "3076b998-62c2-4579-bdf2-8b6cc63dfb38"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_parquet(\"data/driver_stats.parquet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>event_timestamp</th>\n",
              "      <th>driver_id</th>\n",
              "      <th>conv_rate</th>\n",
              "      <th>acc_rate</th>\n",
              "      <th>avg_daily_trips</th>\n",
              "      <th>created</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-11-26 13:00:00+00:00</td>\n",
              "      <td>1005</td>\n",
              "      <td>0.313361</td>\n",
              "      <td>0.610319</td>\n",
              "      <td>321</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-11-26 14:00:00+00:00</td>\n",
              "      <td>1005</td>\n",
              "      <td>0.660652</td>\n",
              "      <td>0.601616</td>\n",
              "      <td>995</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-11-26 15:00:00+00:00</td>\n",
              "      <td>1005</td>\n",
              "      <td>0.165863</td>\n",
              "      <td>0.381302</td>\n",
              "      <td>808</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-11-26 16:00:00+00:00</td>\n",
              "      <td>1005</td>\n",
              "      <td>0.583375</td>\n",
              "      <td>0.237953</td>\n",
              "      <td>913</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-11-26 17:00:00+00:00</td>\n",
              "      <td>1005</td>\n",
              "      <td>0.720630</td>\n",
              "      <td>0.322882</td>\n",
              "      <td>224</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>2021-12-11 11:00:00+00:00</td>\n",
              "      <td>1001</td>\n",
              "      <td>0.752774</td>\n",
              "      <td>0.603291</td>\n",
              "      <td>747</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>2021-12-11 12:00:00+00:00</td>\n",
              "      <td>1001</td>\n",
              "      <td>0.803639</td>\n",
              "      <td>0.674685</td>\n",
              "      <td>541</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>2021-04-12 07:00:00+00:00</td>\n",
              "      <td>1001</td>\n",
              "      <td>0.755037</td>\n",
              "      <td>0.422243</td>\n",
              "      <td>390</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1805</th>\n",
              "      <td>2021-12-04 01:00:00+00:00</td>\n",
              "      <td>1003</td>\n",
              "      <td>0.657862</td>\n",
              "      <td>0.652138</td>\n",
              "      <td>184</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806</th>\n",
              "      <td>2021-12-04 01:00:00+00:00</td>\n",
              "      <td>1003</td>\n",
              "      <td>0.657862</td>\n",
              "      <td>0.652138</td>\n",
              "      <td>184</td>\n",
              "      <td>2021-12-11 13:54:37.581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1807 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               event_timestamp  ...                 created\n",
              "0    2021-11-26 13:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "1    2021-11-26 14:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "2    2021-11-26 15:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "3    2021-11-26 16:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "4    2021-11-26 17:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "...                        ...  ...                     ...\n",
              "1802 2021-12-11 11:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "1803 2021-12-11 12:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "1804 2021-04-12 07:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "1805 2021-12-04 01:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "1806 2021-12-04 01:00:00+00:00  ... 2021-12-11 13:54:37.581\n",
              "\n",
              "[1807 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRL8-ubWzUFy"
      },
      "source": [
        "## Step 3: Register feature definitions and deploy your feature store\n",
        "\n",
        "Now we run `feast apply`to register the feature views and entities defined in `example.py`. The apply command scans Python files in the current directory for feature view/entity definitions, registers the objects, and deploys infrastructure. In this example, it reads `example.py` (shown above) and sets up the Redis online store. Note that we had previously specified Redis as the online store in `feature_store.yaml` (in *Step 2b* above).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYKCKKrcxYZG",
        "outputId": "7ad9c264-55c5-4de8-a328-382dc597ac58"
      },
      "source": [
        "!feast apply"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registered entity \u001b[1m\u001b[32mdriver_id\u001b[0m\n",
            "Registered feature view \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n",
            "Deploying infrastructure for \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV7rtRQgzyf0"
      },
      "source": [
        "## Step 4: Generate training data\n",
        "\n",
        "To train a model, we need features and labels. Often, this label data is stored separately (e.g. you have one table storing user survey results and another set of tables with feature values). \n",
        "\n",
        "The user can query that table of labels with timestamps and pass that into Feast as an *entity dataframe* for training data generation. In many cases, Feast will also intelligently join relevant tables to create the relevant feature vectors.\n",
        "- Note that we include timestamps because want the features for the same driver at various timestamps to be used in a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Fzia7YwBzz",
        "outputId": "5068dc5f-35d5-4d6e-d6c6-92e1220e1b64"
      },
      "source": [
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "from feast import FeatureStore\n",
        "\n",
        "# The entity dataframe is the dataframe we want to enrich with feature values\n",
        "entity_df = pd.DataFrame.from_dict(\n",
        "    {\n",
        "        \"driver_id\": [1001, 1002, 1003],\n",
        "        \"label_driver_reported_satisfaction\": [1, 5, 3], \n",
        "        \"event_timestamp\": [\n",
        "            datetime.now() - timedelta(minutes=11),\n",
        "            datetime.now() - timedelta(minutes=36),\n",
        "            datetime.now() - timedelta(minutes=73),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "store = FeatureStore(repo_path=\".\")\n",
        "\n",
        "training_df = store.get_historical_features(\n",
        "    entity_df=entity_df,\n",
        "    features=[\n",
        "        \"driver_hourly_stats:conv_rate\",\n",
        "        \"driver_hourly_stats:acc_rate\",\n",
        "        \"driver_hourly_stats:avg_daily_trips\",\n",
        "    ],\n",
        ").to_df()\n",
        "\n",
        "print(\"----- Feature schema -----\\n\")\n",
        "print(training_df.info())\n",
        "\n",
        "print()\n",
        "print(\"----- Example features -----\\n\")\n",
        "print(training_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Feature schema -----\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3 entries, 0 to 2\n",
            "Data columns (total 6 columns):\n",
            " #   Column                              Non-Null Count  Dtype              \n",
            "---  ------                              --------------  -----              \n",
            " 0   event_timestamp                     3 non-null      datetime64[ns, UTC]\n",
            " 1   driver_id                           3 non-null      int64              \n",
            " 2   label_driver_reported_satisfaction  3 non-null      int64              \n",
            " 3   conv_rate                           3 non-null      float32            \n",
            " 4   acc_rate                            3 non-null      float32            \n",
            " 5   avg_daily_trips                     3 non-null      int32              \n",
            "dtypes: datetime64[ns, UTC](1), float32(2), int32(1), int64(2)\n",
            "memory usage: 132.0 bytes\n",
            "None\n",
            "\n",
            "----- Example features -----\n",
            "\n",
            "                   event_timestamp  driver_id  ...  acc_rate  avg_daily_trips\n",
            "0 2021-12-11 12:42:22.122271+00:00       1003  ...  0.247415              851\n",
            "1 2021-12-11 13:19:22.122268+00:00       1002  ...  0.475597                0\n",
            "2 2021-12-11 13:44:22.122249+00:00       1001  ...  0.674685              541\n",
            "\n",
            "[3 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCXUpiQ_pmDk"
      },
      "source": [
        "## Step 5: Load features into your Redis online store\n",
        "\n",
        "We will now load or materialize feature data into your Redis online store so we can serve the latest features to models for online prediction. The `materialize` command allows users to materialize features over a specific historical time range into the online store. It will query the batch sources for all feature views over the provided time range, and load the latest feature values into the configured online store. `materialize-incremental` command will only ingest new data that has arrived in the offline store, since the last materialize call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z6QxIebAhK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4d39db-a1a9-4e9f-c7b5-dd50105c21c3"
      },
      "source": [
        "from datetime import datetime\n",
        "!feast materialize-incremental {datetime.now().isoformat()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2021-12-11 13:55:34+00:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
            "\n",
            "\u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m from \u001b[1m\u001b[32m2021-12-10 13:55:35+00:00\u001b[0m to \u001b[1m\u001b[32m2021-12-11 13:55:34+00:00\u001b[0m:\n",
            "\r  0%|                                                                         | 0/5 [00:00<?, ?it/s]\r100%|███████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 2060.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBFlKRsOAhK8"
      },
      "source": [
        "## Step 6: Fetch feature vectors for inference\n",
        "At inference time, we need to quickly read the latest feature values for different drivers (which otherwise might have existed only in batch sources) from the Redis online feature store using `get_online_features()`. These feature vectors can then be fed to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-PUsUWUxoH9",
        "outputId": "a3a713fd-799b-4717-c7a6-a0f2071d7d67"
      },
      "source": [
        "from pprint import pprint\n",
        "from feast import FeatureStore\n",
        "\n",
        "store = FeatureStore(repo_path=\".\")\n",
        "\n",
        "feature_vector = store.get_online_features(\n",
        "    features=[\n",
        "        \"driver_hourly_stats:conv_rate\",\n",
        "        \"driver_hourly_stats:acc_rate\",\n",
        "        \"driver_hourly_stats:avg_daily_trips\",\n",
        "    ],\n",
        "    entity_rows=[\n",
        "        {\"driver_id\": 1004},\n",
        "        {\"driver_id\": 1005},\n",
        "    ],\n",
        ").to_dict()\n",
        "\n",
        "pprint(feature_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acc_rate': [0.36325469613075256, 0.6221743822097778],\n",
            " 'avg_daily_trips': [691, 773],\n",
            " 'conv_rate': [0.6506150364875793, 0.2967696785926819],\n",
            " 'driver_id': [1004, 1005]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg68gH2sy6H1"
      },
      "source": [
        "# Tutorial recap:\n",
        "\n",
        "In this tutorial you’ve deployed a local feature store with a Parquet file offline store and Redis online store. You then built a training dataset using time series features from Parquet files. Then, you materialized feature values from the offline store into the Redis online store. Finally, you read the latest features from the Redis online store for inference. With Redis as the online store you can read the latest feature very quickly for real-time ML use cases, with low latency and high throughput at scale. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSJFUAr0sCTx"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "- Read the Feast [Concepts](https://docs.feast.dev/getting-started/concepts) page to understand the Feast data model, and read the Feast [Architecture](https://docs.feast.dev/getting-started/architecture-and-components) page.\n",
        "- Read the full [configuration](https://rtd.feast.dev/en/master/#module-feast.infra.online_stores.redis) guide for Feast with Redis, and the [data model](https://github.com/feast-dev/feast/blob/master/docs/specs/online_store_format.md) used to store feature values in Redis.\n",
        "- Case studies - learn from your peers: Learn how companies are using Features Stores with Redis as the online store ([Wix](https://youtu.be/E8839ENL-WY), [Swiggy](https://bytes.swiggy.com/enabling-data-science-at-scale-at-swiggy-the-dsp-story-208c2d85faf9), [Comcast](https://cdn.oreillystatic.com/en/assets/1/event/300/Automating%20ML%20model%20training%20and%20deployments%20via%20metadata-driven%20data%2C%20infrastructure%2C%20feature%20engineering%2C%20and%20model%20management%20Presentation.pdf), [Zomato](https://www.zomato.com/blog/elements-of-scalable-machine-learning), [AT&T](https://youtu.be/AXQt_oW9JEc), [DoorDash](https://doordash.engineering/2020/11/19/building-a-gigascale-ml-feature-store-with-redis/), [iFood](https://databricks.com/session_na20/building-a-real-time-feature-store-at-ifood)), and specifically how they are using Feast with Redis for their online store ([Gojek](https://youtu.be/DaNv-Wf1MBA?t=836), [Udaan](https://hasgeek.com/fifthelephant/mlops-conference/schedule/managed-feature-store-improving-data-reusability-providing-a-means-for-low-latency-real-time-prediction-at-udaan-HsZnfC4VUNdWUyJXXwfp5m), [Robinhood](https://www.applyconf.com/agenda/how-robinhood-built-a-feature-store-using-feast/)).\n",
        "- Read about [Azure Managed Feature Store with Feast and Redis](https://github.com/Azure/feast-azure) and follow the [Getting started with Feast on Azure tutorial](https://github.com/Azure/feast-azure/tree/main/provider/tutorial) as well as other Feast tutorials\n",
        "- You can also look for more info on Feast or Redis in the general product introduction pages on [Feast](https://docs.feast.dev/) and [Redis](https://redis.io/topics/introduction) respectively.\n",
        "- Join other Feast users and contributors in [Slack](https://slack.feast.dev) and become part of the community! "
      ]
    }
  ]
}